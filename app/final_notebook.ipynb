{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community fastapi uvicorn nest_asyncio unstructured chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeee612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from unstructured.partition.pdf import partition_pdf\n",
    "# from base64 import b64decode\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# def parse_docs(docs):\n",
    "#     b64 = []\n",
    "#     text = []\n",
    "#     for doc in docs:\n",
    "#         try:\n",
    "#             b64decode(doc)\n",
    "#             b64.append(doc)\n",
    "#         except Exception:\n",
    "#             text.append(doc)\n",
    "#     return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "# def build_prompt(kwargs):\n",
    "#     docs_by_type = kwargs[\"context\"]\n",
    "#     user_question = kwargs[\"question\"]\n",
    "\n",
    "#     context_text = \"\"\n",
    "#     if len(docs_by_type[\"texts\"]) > 0:\n",
    "#         for text_element in docs_by_type[\"texts\"]:\n",
    "#             context_text += str(text_element)\n",
    "\n",
    "#     prompt_template = f\"\"\"\n",
    "#     Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "#     Context: {context_text}\n",
    "#     Question: {user_question}\n",
    "#     \"\"\"\n",
    "#     return prompt_template\n",
    "\n",
    "# def process_pdf_and_answer(question, pdf_path):\n",
    "#     chunks = partition_pdf(\n",
    "#         filename=pdf_path,\n",
    "#         infer_table_structure=True,\n",
    "#         strategy=\"hi_res\",\n",
    "#         extract_image_block_types=[\"image\"],\n",
    "#         extract_image_block_to_payload=True,\n",
    "#         chunking_strategy=\"by_title\",\n",
    "#         max_characters=10000,\n",
    "#         combine_text_under_n_chars=2000,\n",
    "#         new_after_n_chars=6000,\n",
    "#     )\n",
    "\n",
    "#     documents = [Document(page.text, metadata={\"page_number\": page.metadata.page_number}) for page in chunks]\n",
    "\n",
    "#     splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "#     split_docs = splitter.split_documents(documents)\n",
    "\n",
    "#     vectorstore = Chroma.from_documents(split_docs, HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "#     retriever = vectorstore.as_retriever()\n",
    "\n",
    "#     chain = (\n",
    "#         {\n",
    "#             \"context\": retriever | RunnableLambda(parse_docs),\n",
    "#             \"question\": RunnablePassthrough(),\n",
    "#         }\n",
    "#         | RunnableLambda(build_prompt)\n",
    "#         | ChatGroq(model=\"llama3-8b-8192\")\n",
    "#         | StrOutputParser()\n",
    "#     )\n",
    "\n",
    "#     answer = chain.invoke(question)\n",
    "#     return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415db7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastapi import FastAPI, File, UploadFile, Form, Query\n",
    "# from fastapi.responses import JSONResponse\n",
    "# from typing import Optional\n",
    "# import shutil\n",
    "# import os\n",
    "# from rag_pipeline import process_pdf_and_answer\n",
    "\n",
    "# app = FastAPI()\n",
    "\n",
    "# @app.post(\"/chat-with-pdf/\")\n",
    "# async def chat_with_pdf(\n",
    "#     question: str = Form(...),\n",
    "#     pdf_file: Optional[UploadFile] = File(None),\n",
    "#     doc_name: Optional[str] = Query(None)\n",
    "# ):\n",
    "#     if pdf_file:\n",
    "#         file_location = f\"./uploaded_pdfs/{pdf_file.filename}\"\n",
    "#         os.makedirs(os.path.dirname(file_location), exist_ok=True)\n",
    "#         with open(file_location, \"wb\") as f:\n",
    "#             shutil.copyfileobj(pdf_file.file, f)\n",
    "#         response = process_pdf_and_answer(question, file_location)\n",
    "#     elif doc_name:\n",
    "#         existing_file_path = f\"./uploaded_pdfs/{doc_name}\"\n",
    "#         if not os.path.exists(existing_file_path):\n",
    "#             return JSONResponse(status_code=404, content={\"message\": \"Document not found.\"})\n",
    "#         response = process_pdf_and_answer(question, existing_file_path)\n",
    "#     else:\n",
    "#         return JSONResponse(status_code=400, content={\"message\": \"Provide either a PDF file or document name.\"})\n",
    "\n",
    "#     return {\"question\": question, \"answer\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "026521bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from base64 import b64decode\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"  # Replace with your actual key\n",
    "\n",
    "def parse_docs(docs):\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    for text_element in docs_by_type[\"texts\"]:\n",
    "        context_text += str(text_element) + \"\\n\"\n",
    "\n",
    "    # If any base64 images are found, embed them using <img> tag\n",
    "    for image in docs_by_type[\"images\"]:\n",
    "        context_text += f\"\\n[Image]: <img src='data:image/png;base64,{image}' />\"\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "You are an intelligent assistant that understands and interprets both text and visual information (e.g., tables, charts).\n",
    "Answer the following question using the given context, which may contain images embedded in base64.\n",
    "\n",
    "Context: {context_text}\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "def process_pdf_and_answer(question, pdf_path):\n",
    "    chunks = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        infer_table_structure=True,\n",
    "        strategy=\"hi_res\",\n",
    "        extract_image_block_types=[\"image\"],\n",
    "        extract_image_block_to_payload=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=10000,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        new_after_n_chars=6000,\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    for chunk in chunks:\n",
    "        content = chunk.text or \"\"\n",
    "        # Append image data as base64 inside the text\n",
    "        if hasattr(chunk, \"image\") and chunk.image and \"data\" in chunk.image:\n",
    "            image_data = chunk.image[\"data\"]\n",
    "            # Add a marker so we can extract it later\n",
    "            content += f\"\\n[[IMAGE_BASE64]]{image_data}\"\n",
    "        documents.append(Document(content, metadata={\"page_number\": getattr(chunk.metadata, \"page_number\", 0)}))\n",
    "\n",
    "    # Prepare documents for vector search\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    # Store in vector DB\n",
    "    vectorstore = Chroma.from_documents(split_docs, HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    def custom_parser(docs):\n",
    "        b64_images = []\n",
    "        clean_texts = []\n",
    "        for doc in docs:\n",
    "            if \"[[IMAGE_BASE64]]\" in doc:\n",
    "                text_part, *images = doc.split(\"[[IMAGE_BASE64]]\")\n",
    "                clean_texts.append(text_part.strip())\n",
    "                for img in images:\n",
    "                    b64_images.append(img.strip())\n",
    "            else:\n",
    "                clean_texts.append(doc)\n",
    "        return {\"images\": b64_images, \"texts\": clean_texts}\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(custom_parser),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(build_prompt)\n",
    "        | ChatGroq(model=\"llama3-8b-8192\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = chain.invoke(question)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d841fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, Form, Query\n",
    "from fastapi.responses import JSONResponse\n",
    "from typing import Optional\n",
    "import shutil\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat-with-pdf/\")\n",
    "async def chat_with_pdf(\n",
    "    question: str = Form(...),\n",
    "    pdf_file: Optional[UploadFile] = File(None),\n",
    "    doc_name: Optional[str] = Query(None)\n",
    "):\n",
    "    if pdf_file:\n",
    "        file_location = f\"./uploaded_pdfs/{pdf_file.filename}\"\n",
    "        os.makedirs(os.path.dirname(file_location), exist_ok=True)\n",
    "        with open(file_location, \"wb\") as f:\n",
    "            shutil.copyfileobj(pdf_file.file, f)\n",
    "        response = process_pdf_and_answer(question, file_location)\n",
    "    elif doc_name:\n",
    "        existing_file_path = f\"./uploaded_pdfs/{doc_name}\"\n",
    "        if not os.path.exists(existing_file_path):\n",
    "            return JSONResponse(status_code=404, content={\"message\": \"Document not found.\"})\n",
    "        response = process_pdf_and_answer(question, existing_file_path)\n",
    "    else:\n",
    "        return JSONResponse(status_code=400, content={\"message\": \"Provide either a PDF file or document name.\"})\n",
    "\n",
    "    return {\"question\": question, \"answer\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "950ca05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [6472]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:54878 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54878 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54879 - \"POST /chat-with-pdf/?doc_name=Attention.pdf HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:54896 - \"POST /chat-with-pdf/?doc_name=Attention.pdf HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [6472]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"localhost\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
